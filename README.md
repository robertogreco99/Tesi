# How to run the simulations 
## 1. **Build the Podman image**
Run the following command: `podman build -t <imageName>:<tag> <folder>`. 
- `imageName`:  Assigns a name and optionally a tag to the image being built
- `folder`: Specifies the directory containing the Dockerfile and all the required files for the build
This Dockerfile sets up a video quality analysis environment with Python 3.12, FFmpeg 7.0.2, and VMAF 3.0.0. It installs necessary development tools, libraries for video encoding, and Python packages for analysis. The container is configured to run video quality experiments, store results, and generate graphs using pre-defined scripts and MOS data.
## 2. **Modify the configuration file in the JSON folder** 

The configuration file contains several fields:
  - `IMAGE_NAME` : the name of the Docker image used to execute the commands.
  - `INPUT_REF_DIR`: the directory containing reference videos.
  - `INPUT_DIST_DIR`: the directory containing distorted videos, i.e., compressed or altered videos to be compared with the reference videos.
  - `OUTPUT_DIR`: directory where the analysis results, such as JSON files generated by VMAF and graphs, are saved.
  - `HASH_DIR`: Directory for saving hash or checksum files, generated using the md5sum command
  - `MOS_DIR`: Directory where the Mos description files can be found
  - `DATASET_DIR`: Directory where datasets description files can be found
  - `SIMULATIONS_DIR`: Directory where the text files listing the original video files can be found.
  - `JSON_DIR`: Directory where the JSON configuration file and the JSON schema model can be found.
  - `ORIGINAL_VIDEO`: The name of the original YUV or Y4M file to be used as the reference video in the analysis.
  - `MODEL_VERSION` :  Specifies the version or group of VMAF models to be used for the analysis.
    - Example: `VMAF_ALL` indicates that all available VMAF models will be used.
    - Example 2 : `vmaf_v0.6.1.json` indicates that the vmaf will run with that model
  - `DATASET`:  The name of the dataset that will be analyzed
  - `FEATURES`: A list of metrics or features to be calculated during the vmaf analysis.
    - `cambi`: CAMBI (Contrast Aware Multiscale Banding Index) is Netflix's detector for banding (aka contouring) artifacts.
    - `float_ssim`: Structural Similarity Index Metric in floating-point precision.
    - `psnr`: A metric that measures the quality of the image by comparing the peak signal to the noise.
    - `float_ms_ssim`: Multi-Scale SSIM in floating-point precision.
    - `ciede`: CIEDE2000
    - `psnr_hvs`: PSNR optimized for the Human Visual System.
## 3. **Generate Podman commands with the Python script `run_simulation_create_commands.py Json/config.json`:**  
The Python script `run_simulation_create_commands.py` automates the process of generating Podman commands to run VMAF on reference and distorted videos. It is based on the functionality of the `create_commands.py` script and performs several key tasks:
  - Reading a JSON configuration file and validating it against a JSON schema.The file is read to obtain the necessary information for OUTPUT_DIR, DATASET, SIMULATIONS_DIR and JSON_DIR.
  - Updating Configuration: For each original video file listed in a text file (e.g., DATASETNAME_reference_video_list.txt), it updates the JSON configuration file (config.json) by setting the ORIGINAL_VIDEO field to the current YUV file.
  - Executing Command Generation: It calls create_commands.py with the updated JSON configuration, generating Podman commands to process the videos.
  - Cleaning Up: Before generating new commands, it deletes any previous commands_{DATASETNAME} file from earlier runs to ensure
    that the output is up-to-date.
The Podman commands are generated for each original video and saved in `OUTPUT_DIR/{DATASETNAME}/commands_{DATASETNAME}`.
### The python script `create_commands.py`
 The python script `create_commands.py`  generates podman commands to run VMAF on reference and distorted videos. 
 The main functionalities include:
   - Reading a JSON configuration file and validating it against a JSON schema.
   - Comparing the names of distorted videos with the original video's name using regex.
   - Retrieving information (resolution, bitrate, codec, etc.) from a metadata JSON dataset file.
   - Creating commands to run the VMAF script with Podman and saving them to a commands.txt file.
   - Supporting multiple models: Allows the use of one or more VMAF models specified in the configuration.                                                                                                                
  To run the script : `python3 create_commands.py Json/config.json`.
  The output is saved in `OUTPUT_DIR/{DATASETNAME}/commands_{DATASETNAME}`
## 4. **Run the  run_vmaf_simulation.py script to ecxecute the simulation**
  
Every command is a shell command that runs a shell file `run_experiments.sh`.
This script is designed to process video files for quality assessment using a variety of parameters. 
- The script expects 21 arguments, such as directories for reference and distorted videos, output paths, model version, dataset details, hash_dir_server,mos_dir_server and output_dir_server and video properties (e.g., resolution, codec, bitrate). 
- It checks that the necessary directories for input, output, mos and hashes exist, exiting with an error if any are missing.It defines file paths for the original and distorted videos, as well as output paths for decoded and resized versions.
- The script decodes the distorted video using ffmpeg,if the decoded file does not already exist, with different options based on the dataset. It may convert the video to a specific format (e.g., .y4m or .yuv).
    - For ITS4S, decode to YUV420p in a .y4m file.
    - For AGH_NTIA_Dolby, the distorted video is already in .y4m format, so simply copy the file.
    - For AVT-VQDB-UHD-1_1, decode to YUV422p in a .yuv file.
    - For AVT-VQDB-UHD-1_2, AVT-VQDB-UHD-1_3, and AVT-VQDB-UHD-1_4, decode to YUV422p with 10-bit depth in a .yuv file.
    - For KUGVD, GamingVideoSet1, and VideoGamingSet2, decode to YUV420p in a .yuv file.
- Depending on the dataset, it may resize the video to specific dimensions (e.g., 1280x720 or 1920x1080),if the decodedand resized  file does not already exist, using ffmpeg if the current resolution doesn't match the target.
    - For ITS4S and AGH_NTIA_Dolby, resize to 1280x720.
    - For KUGVD, GamingVideoSet1, and GamingVideoSet2, resize to 1920x1080.
    - For AVT-VQDB-UHD-1_1, resize to 4000x2250 for bigbuck_bunny_8bit.yuv related files, and 3840x2160 for others.
    - For AVT-VQDB-UHD-1_2, AVT-VQDB-UHD-1_3, and AVT-VQDB-UHD-1_4, resize to 3840x2160.
- It generates a hash of the decoded video file. 
- The script runs the vmaf simulation. If the model is vmaf_v0.6.1.json, it also runs the feature extraction. The vmaf simulation generates a JSON file as output.
  - VMAF parameters setup:
    - referencevideo: Path to the reference .y4m or .yuv video.
        - For ITS4S, convert the original video to YUV420p.
        - For AVT-VQDB-UHD-1_1, convert the reference video to YUV422 with 8-bit depth if the original video is different from bigbuck_bunny_8bit.yuv.
        - For AVT-VQDB-UHD-1_4, some videos have a different frame rate than the reference; convert the reference video to 30fps or 15fps.
    - distortedvideo: Decoded or decoded and resized distorted video.
    - model: Path to the VMAF model.
    - $feature_args: Used only when the model is vmaf_v0.6.1.json.
    - output: Write output as a JSON file.
    - threads: Specify the number of threads to use for processing.

The simulations generate a `analyzescriptcommands_{DATASETNAME}.txt` file in the `OUTPUT_DIR/{DATASETNAME}` folder. 
The file contains the podman commands to launch in order to create the final csv.
If you want to run simulations on additional datasets, use the Python script `run_more_vmaf_simulation.py`. In this case, you need to set multiple dataset fields and corresponding file_path fields.
  
## 5. **Generate  the final CSV with the script run_create_csv.sh** 
This script checks if a virtual environment named csv_virtual_env exists. If it does not, the script performs the following steps:
  - It creates the virtual environment using python3 -m venv csv_virtual_env.
  - It activates the environment by sourcing its activation script.
  - It installs the required packages listed in the requirements.txt file. (pandas,numpy,scipy and matplotlib)
  - It runs the `run_analyze_script_simulation.py` script with the `Json/config.json` configuration file.
  - After the script finishes running, it deactivates the virtual environment.  
If the virtual environment already exists, the script skips the creation and installation steps, directly activates the environment and runs the run_analyze_script_simulation.py script with the provided configuration file, then deactivates the environment afterward.
The script installs the required packages listed in the requirements.txt file, which includes specific versions of libraries such as pandas, numpy, scipy, and matplotlib. By specifying these versions in the requirements.txt, it ensures that the correct dependencies are installed, providing a consistent environment for the script to run. This helps to avoid compatibility issues and ensures reproducibility across different systems or environments.

### The run_analyze_script_simulation.py 
You can run the script with `run_analyze_script_simulation.py  Json/config.json`
The script reads the JSON configuration file and validates it against a JSON schema. The file is read to obtain the necessary information for `OUTPUT_DIR`, `DATASET`, and `SIMULATIONS_DIR`.R.
The script runs the podman commands that runs `analyze.py` and writes the results of the analysis of the json file output of vmaf on a csv file.
The final csv contains:
  - for every distorted file :
    - for each temporal pooling :
      - the vmaf scores for the 9 vmaf models 
      - the different features values
      - It also has columns vmaf_{model}_bagging,vmaf_{model}_stddev,vmaf_{model}_ci_p95_lo and vmaf_{model}_ci_p95_hi. These columns represent advanced VMAF metrics for the v0.6.3 model, both in floating-point (float) and integer versions:
      - Bagging: Average of the predictions from a lot of  models.
      - Stddev: Standard deviation of predictions
      - CI p95 lo/hi: Lower (lo) and upper (hi) bounds of the 95% confidence interval for the estimated score.
      - integer_vif_scale3

### The python script `analyze.py`
DA RISCRIVERE 
The script 
-  checks if the correct number of arguments are passed (15). These arguments include details about the dataset, video characteristics, and output directory.
- loads a JSON file containing MOS scores for different videos and extracts the MOS, confidence interval (CI), and computed MOS for the distorted video. (Not every dataset has CI)
- It checks if a CSV file with combined results for the dataset exists. If not, it creates the CSV file and adds rows for temporal pooling values (mean, harmonic mean, etc.). If the file exists, it checks if the distorted video is already included; if not, it adds new rows.
- The script constructs a JSON file path based on various parameters (like dataset, video resolution, codec) and loads the results from this file.
- Metrics Calculation: For each frame in the loaded JSON data, the script calculates various metrics, including the mean, harmonic mean, geometric mean, total variation, and norms (L1, L2, L3). It handles missing data by checking for NaN values and prints the results for each metric.
- The most important part of the code comes after the metrics calculation. In this section, the code handles multiple cases for different model versions. For each metric and its corresponding values in metrics_results.items(), the following steps are carried out. First, if the metric is one of the following: vmaf, vmaf_bagging, vmaf_ci_p95_hi, vmaf_ci_p95_lo, or vmaf_stddev, the code removes the JSON extension to match the column name in the DataFrame. Next, the number of rows already occupied in the corresponding column (for a specific model or metric) is calculated. The code then determines the starting row for inserting the new results with `start_row = rows_filled`, where the value of rows_filled defines where the new data will begin to be inserted. The end row is calculated as `end_row = start_row + temporal_pooling_count`, with temporal_pooling_count determining the number of rows that the new results will span. Finally, the code inserts the new metric values (such as mean, harmonic_mean, geometric_mean, etc.) into the existing DataFrame df_existing using `df_existing.loc[start_row:end_row-1, metric] = [mean, harmonic_mean, geometric_mean, percentile_1.percentile_5,percentile_95,  norm_lp_1, norm_lp_2, norm_lp_3]`. The .loc function selects the rows from start_row to end_row-1 (inclusive) and the column corresponding to the metric. The new values are then assigned to this range of rows, completing the insertion of data into the DataFrame.
- The final output includes generated CSV and JSON files containing the computed results and calculated metrics.

## 6. **Run the run_create_graphs.sh script to generate the graphs**
This script checks if a virtual environment named `csv_virtual_env` exists. If it doesn't, it creates the environment, activates it, installs the required packages from `requirements.txt`, and then runs the `graph_simulations_run.py` script with the `Json/config.json` file.
The script installs the required packages listed in the requirements.txt file, which includes specific versions of libraries such as pandas, numpy, scipy, and matplotlib. By specifying these versions in the requirements.txt, it ensures that the correct dependencies are installed, providing a consistent environment for the script to run. This helps to avoid compatibility issues and ensures reproducibility across different systems or environments.
After the script execution, it deactivates the virtual environment. If the virtual environment already exists, the script simply activates it and runs the `graph_simulations_run.py` script without recreating the environment.
###  The graph_simulations_run.py script 
DA RISCRIVERE
To create the graphs, execute the `graph_simulations_run.py` script. This script runs `graph_script.py` from the csv files.
### The python script `graph_script`

The script generates the following
   - for every features all the pvs : for every pvs  different points for the different temporal pooling  : "PVS dir"
   - for every features all the pvs : for every pvs  different points for the different temporal pooling : "PVS dir"
Axis limits are set by a dictionary.
# Requirements
- A JSON dataset file with the following structure:
```json
{
  "database": "DatasetName",
  "reference_videos": [ {
      "id": 1,
      "file_name": "CSGO_30fps_30sec_Part1.yuv"
    },
    ...
  ],
  "distorted_videos": [
    {
      "id": 1,
      "file_name": "CSGO_30fps_30sec_Part2_1280x720_1200_x264.mp4",
      "width": 1280,
      "height": 720,
      "bitrate": 1200,
      "video_codec": "x264",
      "bitdepth": 8,
      "pixel_format": "420",
      "fps": 30,
      "duration": 30
    },
  ]
}
```
- A JSON file with the Mean Opinion Score (MOS). It is essential that the PVS_ID matches the file_name field of the distorted video in the distorted_videos JSON structure. The MOS file can also include additional fields such as Confidence Intervals (CI).
````json
"scores": [
        {
            "PVS": {
                "PVS_ID": "CSGO_30fps_30sec_Part2_1280x720_1200_x264",
                "SRC": "CSGO",
                "fps": 30,
                "duration": 30,
                "parts": "Part2",
                "width": 1280,
                "height": 720,
                "bitrate": 1200.0,
                "encoder": "x264",
                "yuv_fmt": "yuv420p"
            },
            "MOS": 2.88
        },
]
```