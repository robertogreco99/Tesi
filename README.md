# How to run the simulations 
1. Builds the podman image with :  `podman build -t <imageName> <folder`. 
    This Dockerfile sets up a video quality analysis environment with Python 3.12, FFmpeg 7.0.2, and VMAF 3.0.0. It installs necessary development tools, libraries for video encoding, and Python packages for analysis. The container is configured to run video quality experiments, store results, and generate graphs using pre-defined scripts and MOS data.
2. Modify the config file in Json folder    
    There are several fields:
    - `IMAGE_NAME` : the name or tag of the Docker image used to execute the commands.
    - `INPUT_REF_DIR`: the directory containing reference videos
    - `INPUT_DIST_DIR`: the directory containing distorted videos, i.e., compressed or altered videos to be compared with the reference videos.
    - `OUTPUT_DIR`: directory where the analysis results, such as JSON files generated by VMAF and graphs, are saved.
    - `HASH_DIR`: Directory for saving hash or checksum files, generated using the md5sum command
    - `ORIGINAL_VIDEO`: The name of the original YUV or y4m file to be used as the reference video in the analysis.
    - `MODEL_VERSION` :  Specifies the version or group of VMAF models to be used for the analysis.
        - Example: `VMAF_ALL` indicates that all available VMAF models will be used.
        - Example 2 : `vmaf_v0.6.1.json` indicates that the vmaf will run with that model
    - `DATASET`:  The name of the dataset that will be analyzed
    - `FEATURES`: A list of metrics or features to be calculated during the vmaf analysis.
        - `cambi`: CAMBI (Contrast Aware Multiscale Banding Index) is Netflix's detector for banding (aka contouring) artifacts.
        - `float_ssim`: Structural Similarity Index Metric in floating-point precision.
        - `psnr`: A metric that measures the quality of the image by comparing the peak signal to the noise.
        - `float_ms_ssim`: Multi-Scale SSIM in floating-point precision.
        - `ciede`: Color Difference metric (CIEDE2000).
        - `psnr_hvs`: PSNR optimized for the Human Visual System.
2. The python script `create_commands.py`  generates podman commands to run VMAF on reference and distorted videos. The main   functionalities are:
   - Reads a JSON configuration file and validates it against a JSON schema.
   - Compares the names of distorted videos with the original video's name using regex.
   - Retrieves information (resolution, bitrate, codec, etc.) from a metadata JSON dataset file.
   - Creates commands to run the VMAF script with Podman and saves them to a commands.txt file.
   - Support for Multiple Models: Allows usage of one or more VMAF models specified in the configuration.
   - To run the script : `python3 create_commands.py Json/config.json
   - The output is saved in `OUTPUT_DIR/{DATASETNAME}/commands_{DATASETNAME}`
 If you want to run generate all the commands for every original video  in the `INPUT_REF_DIR` run `run_simulation_create_commands.py`. 
 For each original video file listed in a text file (i.e DATASETNAME_reference_video_list.txt), the script: 
   - Updates the JSON configuration file (config.json) by setting the ORIGINAL_VIDEO field to the current YUV file.
   - Executes the create_commands.py script with the updated JSON configuration as input.
Instuction to sets:
    - dataset: Specifies the dataset name.
    - originalvideo_list_file: The path to the text file containing the list of original videos.
    - json_config_path: The path to the JSON configuration file that is updated for each YUV video.
3. Run the `run_vmaf_simulation.py` to run the simulation.
Instruction to set : 
- dataset = Specifies the dataset name.
- file_path = `f"OUTPUT_DIR/{DATASETNAME}/commands_{DATASETNAME}.txt"` : where to found the file with the podman commands
If you want to run more simulations on more dataset run the python `run_more_vmaf_simulation.py`. Here you need to set more dataset fields and file_path fields.
4. The simulations generate a `analyzescriptcommands_{DATASETNAME}.txt` file in the `OUTPUT_DIR/{DATASETNAME}` folder. 
The file contains the podman commands to launch in order to create the final csv.
To run the analysis run `run_analyze_script_simulation.py`.
The csv contains:
- for every distorted file : the vmaf results for the 9 vmaf models and features calculated for different temporal poolings. It has also other 

# What do you need
- A json dataset file description like this :
{
  "database": "DatasetName",
  "reference_videos": [ {
      "id": 1,
      "file_name": "CSGO_30fps_30sec_Part1.yuv"
    },
    ...
  ],
  "distorted_videos": [
    {
      "id": 1,
      "file_name": "CSGO_30fps_30sec_Part2_1280x720_1200_x264.mp4",
      "width": 1280,
      "height": 720,
      "bitrate": 1200,
      "video_codec": "x264",
      "bitdepth": 8,
      "pixel_format": "420",
      "fps": 30,
      "duration": 30
    },
    ..
  ]
}
- A json file with the MOS :
It is important that the PVS_ID matches the file_name field of the distorted video in the distorted_videos json structure-
The mos file can have also more fields like CI
"scores": [
        {
            "PVS": {
                "PVS_ID": "CSGO_30fps_30sec_Part2_1280x720_1200_x264",
                "SRC": "CSGO",
                "fps": 30,
                "duration": 30,
                "parts": "Part2",
                "width": 1280,
                "height": 720,
                "bitrate": 1200.0,
                "encoder": "x264",
                "yuv_fmt": "yuv420p"
            },
            "MOS": 2.88
        },
        ...
]